{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "904dd719-b15f-4665-9661-3433a1241cbf",
   "metadata": {},
   "source": [
    "#### Roadmap\n",
    "- The dataset we will be working with is the LFW dataset.\n",
    "- We will initially try to break it into training and test sets and see the performance of the model\n",
    "- We will then try to use external datasets to train the models and then test it on the LFW dataset\n",
    "    - Try to just use the external dataset as training and then LFW dataset as the test. This should lead to different data distribution problems. Understand hot to solve these\n",
    "    - Then may be use the large external dataset to train embeddings. Then fine tune on the LFW dataset. This should help us understand how to fine tune properly\n",
    "- Then we will look into seeing if we can use the trained models on new faces using webcam. Try to see if we can implement some sort of webcam based lock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e2f6a65-ab5f-49ce-9b30-7222d145a1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eefa56f-0af3-4d8c-8b34-1ac08234501a",
   "metadata": {},
   "source": [
    "#### LFW Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa286ea0-0ce6-4467-8acb-29f73fc65c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a806373-769d-4f2b-bf6c-d71ebed4a986",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairsDataRead:\n",
    "    def __init__(self):\n",
    "        with open('./Data/pairs.txt', 'r') as f:\n",
    "            pairs_data = f.readlines()\n",
    "        self.pairs_data = self.clean_pairs_data(pairs_data)\n",
    "        with open('./Data/pairsDevTest.txt', 'r') as f:\n",
    "            pairs_data_dev = f.readlines()\n",
    "        self.pairs_data_dev = self.clean_pairs_data(pairs_data_dev)\n",
    "        with open('./Data/pairsDevTrain.txt', 'r') as f:\n",
    "            pairs_data_train = f.readlines()\n",
    "        self.pairs_data_train = self.clean_pairs_data(pairs_data_train)\n",
    "\n",
    "    def clean_pairs_data(self, pairs_data):\n",
    "        pairs_list = [pair.strip().split('\\t') for pair in pairs_data]\n",
    "        pairs_list_clean = []\n",
    "        for pair in pairs_list:\n",
    "            pair_clean = []\n",
    "            if len(pair) == 3:\n",
    "                pair_clean.append(pair[0])\n",
    "                pair_clean.append(int(pair[1]))\n",
    "                pair_clean.append(int(pair[2]))\n",
    "            if len(pair) == 4:\n",
    "                pair_clean.append(pair[0])\n",
    "                pair_clean.append(int(pair[1]))\n",
    "                pair_clean.append(pair[2])\n",
    "                pair_clean.append(int(pair[3]))\n",
    "            if pair_clean:\n",
    "                pairs_list_clean.append(pair_clean)\n",
    "        return pairs_list_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7dbb2a28-fd6a-4418-aa8b-f6fba65df8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataRead:\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "        \n",
    "    \n",
    "    def get_image_data(self):\n",
    "        images_data = []\n",
    "        images_path_data = []\n",
    "        for root, dirs, files in os.walk(self.data_dir):\n",
    "            for file in files:\n",
    "                if file.endswith('.jpg') or file.endswith('.png'):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    try:\n",
    "                        with Image.open(file_path) as image:\n",
    "                            image.load()\n",
    "                            image_data = np.asarray(image, dtype = \"int16\")\n",
    "                            images_data.append(image_data)\n",
    "                            images_path_data.append(file_path)\n",
    "                    except:\n",
    "                        print(f\"Unknown error has occured while reading file {file_path}\")\n",
    "        images_data_array = np.stack(images_data, axis = 0)\n",
    "        return images_data_array, images_path_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f1ec8be-ab13-40e4-872b-861b8264791b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_file_n(name, idx):\n",
    "    idx_str = str(idx)\n",
    "    idx_str = '0'*(4 - len(idx_str)) + idx_str\n",
    "    file_name = f\"{name}_{idx_str}.jpg\"\n",
    "    return file_name\n",
    "\n",
    "def get_image(name, idx):\n",
    "    file_name = get_image_file_n(name, idx)\n",
    "    return image_index_dict[file_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12404258-caeb-4743-a2ba-80db959144b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LFWDataset:\n",
    "    def __init__(self, image_data_dir):\n",
    "        self.pdr = PairsDataRead()\n",
    "        self.idr = ImageDataRead(image_data_dir)\n",
    "        images_data_array, images_path_data = self.idr.get_image_data()\n",
    "        self.image_1_data, self.image_2_data, self.labels = self._process_image_data(images_data_array, images_path_data)\n",
    "\n",
    "    def _process_image_data(self, images_data_array, images_path_data):\n",
    "        image_index_dict = defaultdict(lambda : None)\n",
    "        character_index_dict = defaultdict(lambda : [])\n",
    "        for idx, image_path in enumerate(images_path_data):\n",
    "            image_name = image_path.split('\\\\')[-1]\n",
    "            character_name = image_path.split('\\\\')[-2]\n",
    "            image_index = idx\n",
    "            image_index_dict[image_name] = image_index\n",
    "            character_index_dict[character_name].append(image_index)\n",
    "        \n",
    "        image_1_index = [image_index_dict[get_image_file_n(pair[0], pair[1])] for pair in pdr.pairs_data_train]\n",
    "        image_2_index = [\n",
    "            image_index_dict[get_image_file_n(pair[0], pair[2])] if len(pair) == 3 \n",
    "            else image_index_dict[get_image_file_n(pair[2], pair[3])] for pair in pdr.pairs_data_train\n",
    "        ]\n",
    "        matching_label = [\n",
    "            1 if len(pair) == 3\n",
    "            else 0 \n",
    "            for pair in pdr.pairs_data_train\n",
    "        ]\n",
    "        image_1_data = images_data_array[image_1_index]\n",
    "        image_2_data = images_data_array[image_2_index]\n",
    "        labels = np.array(matching_label)\n",
    "        return image_1_data, image_2_data, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.image_1_data[idx], self.image_2_data[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cee2c7-89dc-46dc-8e1a-7c5d001058c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223837f7-3d9e-41ea-8cd4-dc99b0494570",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6900b2b7-239a-4cb4-997f-2c895fea75ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_file_n(name, idx):\n",
    "    idx_str = str(idx)\n",
    "    idx_str = '0'*(4 - len(idx_str)) + idx_str\n",
    "    file_name = f\"{name}_{idx_str}.jpg\"\n",
    "    return file_name\n",
    "\n",
    "def get_image(name, idx):\n",
    "    file_name = get_image_file_n(name, idx)\n",
    "    return image_index_dict[file_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cc6dd6d-2b1f-4acd-8d5a-26edc4caa6e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1\\\\2'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{1}\\{2}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3110df8-129e-4f5d-8fac-5605967df74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_1 = np.random.rand(10,250,250,3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94fc5896-eba0-4d45-921a-d03d2f75d46a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 3, 250, 250)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_1.transpose(0,3,1,2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "5d0c89c0-1dd7-452b-aaff-a1c4ed72b850",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LFWDatasetSlow:\n",
    "    def __init__(self, image_data_dir, train = True, cache = True, transform = None, label_transform = None):\n",
    "        self.pairs_data = PairsDataRead()\n",
    "        self.image_data_dir = image_data_dir\n",
    "        self.train = train\n",
    "        self.cache = cache\n",
    "        self.transform = transform\n",
    "        self.label_transform = label_transform\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        if self.train:\n",
    "            return len(self.pairs_data.pairs_data_train)\n",
    "        else:\n",
    "            return len(self.pairs_data.pairs_data_dev)\n",
    "\n",
    "    def _get_image_from_name_idx(self, name, idx):\n",
    "        idx_str = str(idx)\n",
    "        idx_str = '0'*(4 - len(idx_str)) + idx_str\n",
    "        file_name = f\"{name}_{idx_str}.jpg\"\n",
    "        file_path = f\"{self.image_data_dir}\\{name}\\{file_name}\"\n",
    "        img = Image.open(file_path)\n",
    "        return np.array(img)\n",
    "\n",
    "    \n",
    "    def _get_image_label_pair(self, pair):\n",
    "        if len(pair) == 3:\n",
    "            label = 1\n",
    "            img_1_name = pair[0]\n",
    "            img_2_name = pair[0]\n",
    "            img_1_idx = pair[1]\n",
    "            img_2_idx = pair[2]\n",
    "        else:\n",
    "            label = 0\n",
    "            img_1_name = pair[0]\n",
    "            img_2_name = pair[2]\n",
    "            img_1_idx = pair[1]\n",
    "            img_2_idx = pair[3]\n",
    "        img_1 = self._get_image_from_name_idx(img_1_name, img_1_idx)\n",
    "        img_2 = self._get_image_from_name_idx(img_2_name, img_2_idx)\n",
    "        if self.transform:\n",
    "            img_1 = self.transform(img_1)\n",
    "            img_2 = self.transform(img_2)\n",
    "        if self.label_transform:\n",
    "            label = self.label_transform(label)\n",
    "        \n",
    "        # img_1 = img_1.transpose(2, 0, 1)\n",
    "        # img_2 = img_2.transpose(2, 0, 1)\n",
    "        # img_1 = img_1.astype(np.float32)\n",
    "        # img_2 = img_2.astype(np.float32)\n",
    "        # label = np.array(label).astype(np.float32)\n",
    "        return img_1, img_2, label\n",
    "\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        if self.train:\n",
    "            pairs_list = self.pairs_data.pairs_data_train\n",
    "        else:\n",
    "            pairs_list = self.pairs_data.pairs_data_dev\n",
    "        pair = pairs_list[idx]\n",
    "        return self._get_image_label_pair(pair)\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6818ffcd-55f4-449f-9606-d1d499d93bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "abdd8412-92c2-4c36-9baf-a577047edb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "c702558c-b685-4dda-a1ae-ed94bdf4e4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize image\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "0cf00f2d-aa51-4eb0-81ff-b4d8a2cd7821",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lfw_data = LFWDatasetSlow(\".\\Data\\lfw_funneled\", train = True, transform = custom_transform, label_transform = torch.tensor)\n",
    "dev_lfw_data = LFWDatasetSlow(\".\\Data\\lfw_funneled\", train = False, transform = custom_transform, label_transform = torch.tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "e6c3a061-6b01-4957-8c13-ce57d4f9d2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_lfw_data, batch_size = 64, shuffle = True)\n",
    "dev_dataloader = DataLoader(dev_lfw_data, batch_size = 64, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10171866-554a-4fbf-ad75-7fcd626fc4c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7f8d33-9ee8-4728-841b-1a7f15bc9a0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88f2d9f-f2b2-4aca-920b-85bcc33c0e4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb3f465-b8f1-4346-9eaa-b2babef27874",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "692e67cd-5bbc-47a2-9975-a050a67bfca3",
   "metadata": {},
   "source": [
    "##### We now have our initial dataset\n",
    "- Build a resnet or some other standard vision model and train it on the data\n",
    "- Build a test set similarly\n",
    "- Test the performance on the test set\n",
    "- Try to see how to use pretrained image based models here\n",
    "  - Here one problem is how to deal with different resolutions of your images and the images on which the pretrained models were trained.\n",
    "  - We can then just use cosine similarity and check performance.\n",
    "  - We can also try to finetune the pre trained model properly and check performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56b89fa2-3cd6-43dc-9a03-6caab1185f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3da413a-5976-4f54-bf8b-c52f14b78783",
   "metadata": {},
   "source": [
    "#### A simple siamese style classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "0fe9c516-eb7a-4183-b3a2-8cd6e10bdde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.model_chunk = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 5, padding = 2),\n",
    "            nn.ReLU(),\n",
    "            # nn.Conv2d(16, 32, 5, padding = 1),\n",
    "            # nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            # nn.Conv2d(32, 16, 5, padding = 1),\n",
    "            # nn.ReLU(),\n",
    "            nn.Conv2d(16, 8, 5, padding = 2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            # nn.Conv2d(8, 8, 5, padding = 1),\n",
    "            # nn.ReLU(),\n",
    "            nn.Conv2d(8, 4, 5, padding = 2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(4, 4, 5, padding = 2),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(900, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128)\n",
    "        )\n",
    "    def forward(self, img):\n",
    "        return self.model_chunk(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "dacd103d-cc5e-40cc-a065-c83fa15a3179",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "2192bf00-af0f-4dd8-a898-008e805a6bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, in_channels, m = 0.5):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.m = m\n",
    "        self.lenet_block = LeNet(in_channels)\n",
    "        self.fc1 = nn.Linear(512, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 1)\n",
    "        \n",
    "    def forward(self, img1, img2):\n",
    "        img1_emb = self.lenet_block(img1)\n",
    "        img2_emb = self.lenet_block(img2)\n",
    "        stacked_layer = torch.hstack([img1_emb, img2_emb, img1_emb + img2_emb, img1_emb - img2_emb])\n",
    "        output = F.relu(self.fc1(stacked_layer))\n",
    "        output = self.fc2(output)\n",
    "        output = F.sigmoid(self.fc3(output))\n",
    "        return output\n",
    "\n",
    "    # def cosine_similarity(self, img1_emb, img2_emb):\n",
    "    #     emb1_mod = torch.sqrt((img1_emb * img1_emb).sum(dim = 1))\n",
    "    #     emb2_mod = torch.sqrt((img2_emb * img2_emb).sum(dim = 1))\n",
    "    #     emb_dot = (img1_emb * img2_emb).sum(dim = 1)\n",
    "    #     cosine_similarity = emb_dot/(emb1_mod * emb2_mod)\n",
    "    #     return cosine_similarity\n",
    "\n",
    "    # def cosine_similarity_adj(self, img1_emb, img2_emb):\n",
    "    #     cosine_similarity = self.cosine_similarity(img1_emb, img2_emb)\n",
    "    #     cosine_similarity = (cosine_similarity + 1)/2\n",
    "    #     return cosine_similarity\n",
    "    \n",
    "    # def loss(self, img1_emb, img2_emb, label):\n",
    "    #     cosine_similarity = self.cosine_similarity(img1_emb, img2_emb)\n",
    "    #     cosine_distance = torch.pow((1 - cosine_similarity)/2, 2)\n",
    "    #     contrasive_loss_1 = label*torch.pow((1 - cosine_distance), 2)\n",
    "    #     contrasive_loss_2 = (1-label)*torch.pow(torch.clip(self.m - cosine_distance, min = 0), 2)\n",
    "    #     contrasive_loss = contrasive_loss_1 + contrasive_loss_2\n",
    "    #     contrasive_loss = torch.mean(contrasive_loss)\n",
    "    #     return contrasive_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086486a4-69e0-4496-a111-4f114f121a11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "0201177d-07dc-4dfe-a595-40bf9ed14c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "022e9ea4-794a-4a25-b23a-20f06e6cfa4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minibatch - 0, loss - 0.6866062879562378, accurate predictions - 37, total predictions - 64\n",
      "minibatch - 1, loss - 0.7219350337982178, accurate predictions - 30, total predictions - 64\n",
      "minibatch - 2, loss - 0.6995278000831604, accurate predictions - 29, total predictions - 64\n",
      "minibatch - 3, loss - 0.6861046552658081, accurate predictions - 38, total predictions - 64\n",
      "minibatch - 4, loss - 0.7254888415336609, accurate predictions - 31, total predictions - 64\n",
      "minibatch - 5, loss - 0.6959218978881836, accurate predictions - 33, total predictions - 64\n",
      "minibatch - 6, loss - 0.6942237615585327, accurate predictions - 31, total predictions - 64\n",
      "minibatch - 7, loss - 0.6868767142295837, accurate predictions - 38, total predictions - 64\n",
      "minibatch - 8, loss - 0.7019878029823303, accurate predictions - 31, total predictions - 64\n",
      "minibatch - 9, loss - 0.6797695159912109, accurate predictions - 38, total predictions - 64\n",
      "minibatch - 10, loss - 0.712787389755249, accurate predictions - 28, total predictions - 64\n",
      "minibatch - 11, loss - 0.695324182510376, accurate predictions - 32, total predictions - 64\n",
      "minibatch - 12, loss - 0.6872286796569824, accurate predictions - 38, total predictions - 64\n",
      "minibatch - 13, loss - 0.6869767904281616, accurate predictions - 39, total predictions - 64\n",
      "minibatch - 14, loss - 0.6879414319992065, accurate predictions - 37, total predictions - 64\n",
      "minibatch - 15, loss - 0.6971374750137329, accurate predictions - 30, total predictions - 64\n",
      "minibatch - 16, loss - 0.680588960647583, accurate predictions - 41, total predictions - 64\n",
      "minibatch - 17, loss - 0.7018211483955383, accurate predictions - 28, total predictions - 64\n",
      "minibatch - 18, loss - 0.6930233836174011, accurate predictions - 33, total predictions - 64\n",
      "minibatch - 19, loss - 0.6984363794326782, accurate predictions - 30, total predictions - 64\n",
      "minibatch - 20, loss - 0.6929051876068115, accurate predictions - 33, total predictions - 64\n",
      "minibatch - 21, loss - 0.7025477886199951, accurate predictions - 27, total predictions - 64\n",
      "minibatch - 22, loss - 0.6941550970077515, accurate predictions - 32, total predictions - 64\n",
      "minibatch - 23, loss - 0.6997866630554199, accurate predictions - 27, total predictions - 64\n",
      "minibatch - 24, loss - 0.6908009052276611, accurate predictions - 35, total predictions - 64\n",
      "minibatch - 25, loss - 0.6911524534225464, accurate predictions - 35, total predictions - 64\n",
      "minibatch - 26, loss - 0.6940245628356934, accurate predictions - 31, total predictions - 64\n",
      "minibatch - 27, loss - 0.6932910680770874, accurate predictions - 32, total predictions - 64\n",
      "minibatch - 28, loss - 0.6985198259353638, accurate predictions - 20, total predictions - 64\n",
      "minibatch - 29, loss - 0.6933504343032837, accurate predictions - 30, total predictions - 64\n",
      "minibatch - 30, loss - 0.694121778011322, accurate predictions - 29, total predictions - 64\n",
      "minibatch - 31, loss - 0.694661021232605, accurate predictions - 30, total predictions - 64\n",
      "minibatch - 32, loss - 0.6887892484664917, accurate predictions - 40, total predictions - 64\n",
      "minibatch - 33, loss - 0.6913714408874512, accurate predictions - 34, total predictions - 64\n",
      "minibatch - 34, loss - 0.6988469958305359, accurate predictions - 11, total predictions - 24\n",
      "epoch - 0, loss - 0.7158246040344238\n",
      "minibatch - 0, loss - 0.7044680118560791, accurate predictions - 26, total predictions - 64\n",
      "minibatch - 1, loss - 0.6899749040603638, accurate predictions - 35, total predictions - 64\n",
      "minibatch - 2, loss - 0.69739830493927, accurate predictions - 29, total predictions - 64\n",
      "minibatch - 3, loss - 0.6962172985076904, accurate predictions - 29, total predictions - 64\n",
      "minibatch - 4, loss - 0.6943861246109009, accurate predictions - 30, total predictions - 64\n",
      "minibatch - 5, loss - 0.6927619576454163, accurate predictions - 34, total predictions - 64\n",
      "minibatch - 6, loss - 0.6931541562080383, accurate predictions - 31, total predictions - 64\n",
      "minibatch - 7, loss - 0.692606508731842, accurate predictions - 36, total predictions - 64\n",
      "minibatch - 8, loss - 0.6925444602966309, accurate predictions - 34, total predictions - 64\n",
      "minibatch - 9, loss - 0.6943166255950928, accurate predictions - 30, total predictions - 64\n",
      "minibatch - 10, loss - 0.6897072792053223, accurate predictions - 38, total predictions - 64\n",
      "minibatch - 11, loss - 0.6919298768043518, accurate predictions - 34, total predictions - 64\n",
      "minibatch - 12, loss - 0.6917484402656555, accurate predictions - 34, total predictions - 64\n",
      "minibatch - 13, loss - 0.6981207132339478, accurate predictions - 28, total predictions - 64\n",
      "minibatch - 14, loss - 0.6850621700286865, accurate predictions - 40, total predictions - 64\n",
      "minibatch - 15, loss - 0.6926767826080322, accurate predictions - 33, total predictions - 64\n",
      "minibatch - 16, loss - 0.692799985408783, accurate predictions - 33, total predictions - 64\n",
      "minibatch - 17, loss - 0.7066851258277893, accurate predictions - 25, total predictions - 64\n",
      "minibatch - 18, loss - 0.7022082805633545, accurate predictions - 26, total predictions - 64\n",
      "minibatch - 19, loss - 0.6954619884490967, accurate predictions - 30, total predictions - 64\n",
      "minibatch - 20, loss - 0.6915483474731445, accurate predictions - 35, total predictions - 64\n",
      "minibatch - 21, loss - 0.6928418874740601, accurate predictions - 33, total predictions - 64\n",
      "minibatch - 22, loss - 0.694172203540802, accurate predictions - 29, total predictions - 64\n",
      "minibatch - 23, loss - 0.6935436725616455, accurate predictions - 29, total predictions - 64\n",
      "minibatch - 24, loss - 0.6921898126602173, accurate predictions - 37, total predictions - 64\n",
      "minibatch - 25, loss - 0.6944564580917358, accurate predictions - 31, total predictions - 64\n",
      "minibatch - 26, loss - 0.6887772083282471, accurate predictions - 36, total predictions - 64\n",
      "minibatch - 27, loss - 0.705111026763916, accurate predictions - 28, total predictions - 64\n",
      "minibatch - 28, loss - 0.696819543838501, accurate predictions - 31, total predictions - 64\n",
      "minibatch - 29, loss - 0.6914252042770386, accurate predictions - 34, total predictions - 64\n",
      "minibatch - 30, loss - 0.6943551898002625, accurate predictions - 31, total predictions - 64\n",
      "minibatch - 31, loss - 0.6928141713142395, accurate predictions - 33, total predictions - 64\n",
      "minibatch - 32, loss - 0.6928936243057251, accurate predictions - 34, total predictions - 64\n",
      "minibatch - 33, loss - 0.6930844187736511, accurate predictions - 34, total predictions - 64\n",
      "minibatch - 34, loss - 0.6950871348381042, accurate predictions - 7, total predictions - 24\n",
      "epoch - 1, loss - 0.7148044109344482\n",
      "minibatch - 0, loss - 0.693213701248169, accurate predictions - 31, total predictions - 64\n",
      "minibatch - 1, loss - 0.6933889985084534, accurate predictions - 31, total predictions - 64\n",
      "minibatch - 2, loss - 0.6911514401435852, accurate predictions - 37, total predictions - 64\n",
      "minibatch - 3, loss - 0.6944674253463745, accurate predictions - 30, total predictions - 64\n",
      "minibatch - 4, loss - 0.6949199438095093, accurate predictions - 29, total predictions - 64\n",
      "minibatch - 5, loss - 0.6931390762329102, accurate predictions - 32, total predictions - 64\n",
      "minibatch - 6, loss - 0.6924924254417419, accurate predictions - 33, total predictions - 64\n",
      "minibatch - 7, loss - 0.6923364400863647, accurate predictions - 34, total predictions - 64\n",
      "minibatch - 8, loss - 0.6932815313339233, accurate predictions - 29, total predictions - 64\n",
      "minibatch - 9, loss - 0.6915003061294556, accurate predictions - 37, total predictions - 64\n",
      "minibatch - 10, loss - 0.6931120753288269, accurate predictions - 34, total predictions - 64\n",
      "minibatch - 11, loss - 0.6942130327224731, accurate predictions - 30, total predictions - 64\n",
      "minibatch - 12, loss - 0.6945560574531555, accurate predictions - 35, total predictions - 64\n",
      "minibatch - 13, loss - 0.7016921639442444, accurate predictions - 25, total predictions - 64\n",
      "minibatch - 14, loss - 0.6948720216751099, accurate predictions - 32, total predictions - 64\n",
      "minibatch - 15, loss - 0.6952158808708191, accurate predictions - 26, total predictions - 64\n",
      "minibatch - 16, loss - 0.6920382380485535, accurate predictions - 33, total predictions - 64\n",
      "minibatch - 17, loss - 0.6926974058151245, accurate predictions - 34, total predictions - 64\n",
      "minibatch - 18, loss - 0.6927943229675293, accurate predictions - 32, total predictions - 64\n",
      "minibatch - 19, loss - 0.6935383081436157, accurate predictions - 25, total predictions - 64\n",
      "minibatch - 20, loss - 0.6937676668167114, accurate predictions - 28, total predictions - 64\n",
      "minibatch - 21, loss - 0.6928558945655823, accurate predictions - 32, total predictions - 64\n",
      "minibatch - 22, loss - 0.693008303642273, accurate predictions - 32, total predictions - 64\n",
      "minibatch - 23, loss - 0.6933839321136475, accurate predictions - 26, total predictions - 64\n",
      "minibatch - 24, loss - 0.6935363411903381, accurate predictions - 27, total predictions - 64\n",
      "minibatch - 25, loss - 0.6930986642837524, accurate predictions - 32, total predictions - 64\n",
      "minibatch - 26, loss - 0.692916750907898, accurate predictions - 34, total predictions - 64\n",
      "minibatch - 27, loss - 0.6933667659759521, accurate predictions - 29, total predictions - 64\n",
      "minibatch - 28, loss - 0.6930918097496033, accurate predictions - 36, total predictions - 64\n",
      "minibatch - 29, loss - 0.6932225227355957, accurate predictions - 29, total predictions - 64\n",
      "minibatch - 30, loss - 0.6930753588676453, accurate predictions - 43, total predictions - 64\n",
      "minibatch - 31, loss - 0.6932470202445984, accurate predictions - 30, total predictions - 64\n",
      "minibatch - 32, loss - 0.6933122873306274, accurate predictions - 27, total predictions - 64\n",
      "minibatch - 33, loss - 0.6929845809936523, accurate predictions - 36, total predictions - 64\n",
      "minibatch - 34, loss - 0.6912147402763367, accurate predictions - 16, total predictions - 24\n",
      "epoch - 2, loss - 0.7138442993164062\n",
      "minibatch - 0, loss - 0.6969220042228699, accurate predictions - 26, total predictions - 64\n",
      "minibatch - 1, loss - 0.6957325339317322, accurate predictions - 29, total predictions - 64\n",
      "minibatch - 2, loss - 0.6910898685455322, accurate predictions - 35, total predictions - 64\n",
      "minibatch - 3, loss - 0.688058614730835, accurate predictions - 38, total predictions - 64\n",
      "minibatch - 4, loss - 0.6998468637466431, accurate predictions - 27, total predictions - 64\n",
      "minibatch - 5, loss - 0.6938046216964722, accurate predictions - 32, total predictions - 64\n",
      "minibatch - 6, loss - 0.6914736032485962, accurate predictions - 34, total predictions - 64\n",
      "minibatch - 7, loss - 0.6926266551017761, accurate predictions - 33, total predictions - 64\n",
      "minibatch - 8, loss - 0.6949312686920166, accurate predictions - 31, total predictions - 64\n",
      "minibatch - 9, loss - 0.696652889251709, accurate predictions - 29, total predictions - 64\n",
      "minibatch - 10, loss - 0.6918563842773438, accurate predictions - 34, total predictions - 64\n",
      "minibatch - 11, loss - 0.6932758092880249, accurate predictions - 32, total predictions - 64\n",
      "minibatch - 12, loss - 0.6947011947631836, accurate predictions - 29, total predictions - 64\n",
      "minibatch - 13, loss - 0.6919218301773071, accurate predictions - 36, total predictions - 64\n",
      "minibatch - 14, loss - 0.693111002445221, accurate predictions - 32, total predictions - 64\n",
      "minibatch - 15, loss - 0.6930171847343445, accurate predictions - 33, total predictions - 64\n",
      "minibatch - 16, loss - 0.6932646632194519, accurate predictions - 31, total predictions - 64\n",
      "minibatch - 17, loss - 0.6933165192604065, accurate predictions - 29, total predictions - 64\n",
      "minibatch - 18, loss - 0.6930703520774841, accurate predictions - 36, total predictions - 64\n",
      "minibatch - 19, loss - 0.6922770738601685, accurate predictions - 39, total predictions - 64\n",
      "minibatch - 20, loss - 0.6917381286621094, accurate predictions - 37, total predictions - 64\n",
      "minibatch - 21, loss - 0.6932730674743652, accurate predictions - 32, total predictions - 64\n",
      "minibatch - 22, loss - 0.6959027051925659, accurate predictions - 28, total predictions - 64\n",
      "minibatch - 23, loss - 0.6919620037078857, accurate predictions - 34, total predictions - 64\n",
      "minibatch - 24, loss - 0.6934318542480469, accurate predictions - 32, total predictions - 64\n",
      "minibatch - 25, loss - 0.6926494836807251, accurate predictions - 33, total predictions - 64\n",
      "minibatch - 26, loss - 0.6999293565750122, accurate predictions - 25, total predictions - 64\n",
      "minibatch - 27, loss - 0.6979410648345947, accurate predictions - 27, total predictions - 64\n",
      "minibatch - 28, loss - 0.6894809007644653, accurate predictions - 37, total predictions - 64\n",
      "minibatch - 29, loss - 0.6926802396774292, accurate predictions - 33, total predictions - 64\n",
      "minibatch - 30, loss - 0.6963571906089783, accurate predictions - 28, total predictions - 64\n",
      "minibatch - 31, loss - 0.6961020231246948, accurate predictions - 28, total predictions - 64\n",
      "minibatch - 32, loss - 0.6939658522605896, accurate predictions - 31, total predictions - 64\n",
      "minibatch - 33, loss - 0.6971361041069031, accurate predictions - 25, total predictions - 64\n",
      "minibatch - 34, loss - 0.6920967102050781, accurate predictions - 13, total predictions - 24\n",
      "epoch - 3, loss - 0.7142823934555054\n",
      "minibatch - 0, loss - 0.690058708190918, accurate predictions - 41, total predictions - 64\n",
      "minibatch - 1, loss - 0.6949609518051147, accurate predictions - 27, total predictions - 64\n",
      "minibatch - 2, loss - 0.6913986206054688, accurate predictions - 38, total predictions - 64\n",
      "minibatch - 3, loss - 0.6922963857650757, accurate predictions - 35, total predictions - 64\n",
      "minibatch - 4, loss - 0.6966561079025269, accurate predictions - 21, total predictions - 64\n",
      "minibatch - 5, loss - 0.6931841373443604, accurate predictions - 32, total predictions - 64\n",
      "minibatch - 6, loss - 0.6923685073852539, accurate predictions - 36, total predictions - 64\n",
      "minibatch - 7, loss - 0.6940310001373291, accurate predictions - 27, total predictions - 64\n",
      "minibatch - 8, loss - 0.6925894021987915, accurate predictions - 37, total predictions - 64\n",
      "minibatch - 9, loss - 0.6926625967025757, accurate predictions - 37, total predictions - 64\n",
      "minibatch - 10, loss - 0.6932463645935059, accurate predictions - 31, total predictions - 64\n",
      "minibatch - 11, loss - 0.6934036612510681, accurate predictions - 30, total predictions - 64\n",
      "minibatch - 12, loss - 0.692566990852356, accurate predictions - 37, total predictions - 64\n",
      "minibatch - 13, loss - 0.6928632259368896, accurate predictions - 34, total predictions - 64\n",
      "minibatch - 14, loss - 0.6933170557022095, accurate predictions - 31, total predictions - 64\n",
      "minibatch - 15, loss - 0.6939961910247803, accurate predictions - 28, total predictions - 64\n",
      "minibatch - 16, loss - 0.6925555467605591, accurate predictions - 35, total predictions - 64\n",
      "minibatch - 17, loss - 0.6933732032775879, accurate predictions - 31, total predictions - 64\n",
      "minibatch - 18, loss - 0.6916623115539551, accurate predictions - 39, total predictions - 64\n",
      "minibatch - 19, loss - 0.6936966776847839, accurate predictions - 30, total predictions - 64\n",
      "minibatch - 20, loss - 0.6937569379806519, accurate predictions - 30, total predictions - 64\n",
      "minibatch - 21, loss - 0.6946494579315186, accurate predictions - 27, total predictions - 64\n",
      "minibatch - 22, loss - 0.6931862831115723, accurate predictions - 32, total predictions - 64\n",
      "minibatch - 23, loss - 0.6944444179534912, accurate predictions - 27, total predictions - 64\n",
      "minibatch - 24, loss - 0.6939831376075745, accurate predictions - 28, total predictions - 64\n",
      "minibatch - 25, loss - 0.6928894519805908, accurate predictions - 34, total predictions - 64\n",
      "minibatch - 26, loss - 0.6928094029426575, accurate predictions - 36, total predictions - 64\n",
      "minibatch - 27, loss - 0.693012535572052, accurate predictions - 34, total predictions - 64\n",
      "minibatch - 28, loss - 0.6932027339935303, accurate predictions - 31, total predictions - 64\n",
      "minibatch - 29, loss - 0.6930640935897827, accurate predictions - 34, total predictions - 64\n",
      "minibatch - 30, loss - 0.6932004690170288, accurate predictions - 30, total predictions - 64\n",
      "minibatch - 31, loss - 0.6932164430618286, accurate predictions - 28, total predictions - 64\n",
      "minibatch - 32, loss - 0.693050742149353, accurate predictions - 39, total predictions - 64\n",
      "minibatch - 33, loss - 0.6932399868965149, accurate predictions - 31, total predictions - 64\n",
      "minibatch - 34, loss - 0.6939796209335327, accurate predictions - 10, total predictions - 24\n",
      "epoch - 4, loss - 0.7136051058769226\n",
      "minibatch - 0, loss - 0.6931552886962891, accurate predictions - 32, total predictions - 64\n",
      "minibatch - 1, loss - 0.6935850977897644, accurate predictions - 30, total predictions - 64\n",
      "minibatch - 2, loss - 0.6925592422485352, accurate predictions - 35, total predictions - 64\n",
      "minibatch - 3, loss - 0.6927216053009033, accurate predictions - 34, total predictions - 64\n",
      "minibatch - 4, loss - 0.6934245228767395, accurate predictions - 31, total predictions - 64\n",
      "minibatch - 5, loss - 0.6928966641426086, accurate predictions - 33, total predictions - 64\n",
      "minibatch - 6, loss - 0.6960722208023071, accurate predictions - 22, total predictions - 64\n",
      "minibatch - 7, loss - 0.6938594579696655, accurate predictions - 29, total predictions - 64\n",
      "minibatch - 8, loss - 0.6937863826751709, accurate predictions - 28, total predictions - 64\n",
      "minibatch - 9, loss - 0.6933629512786865, accurate predictions - 29, total predictions - 64\n",
      "minibatch - 10, loss - 0.6931290626525879, accurate predictions - 32, total predictions - 64\n",
      "minibatch - 11, loss - 0.6934762001037598, accurate predictions - 29, total predictions - 64\n",
      "minibatch - 12, loss - 0.6933183670043945, accurate predictions - 31, total predictions - 64\n",
      "minibatch - 13, loss - 0.69251549243927, accurate predictions - 35, total predictions - 64\n",
      "minibatch - 14, loss - 0.6947265267372131, accurate predictions - 26, total predictions - 64\n",
      "minibatch - 15, loss - 0.693707287311554, accurate predictions - 30, total predictions - 64\n",
      "minibatch - 16, loss - 0.6945294141769409, accurate predictions - 27, total predictions - 64\n",
      "minibatch - 17, loss - 0.6926984786987305, accurate predictions - 34, total predictions - 64\n",
      "minibatch - 18, loss - 0.6927182674407959, accurate predictions - 34, total predictions - 64\n",
      "minibatch - 19, loss - 0.6935911178588867, accurate predictions - 30, total predictions - 64\n",
      "minibatch - 20, loss - 0.6904696226119995, accurate predictions - 45, total predictions - 64\n",
      "minibatch - 21, loss - 0.6931817531585693, accurate predictions - 32, total predictions - 64\n",
      "minibatch - 22, loss - 0.6940658092498779, accurate predictions - 29, total predictions - 64\n",
      "minibatch - 23, loss - 0.6922481060028076, accurate predictions - 35, total predictions - 64\n",
      "minibatch - 24, loss - 0.6921448707580566, accurate predictions - 35, total predictions - 64\n",
      "minibatch - 25, loss - 0.6947867274284363, accurate predictions - 28, total predictions - 64\n",
      "minibatch - 26, loss - 0.6956758499145508, accurate predictions - 26, total predictions - 64\n",
      "minibatch - 27, loss - 0.6963592171669006, accurate predictions - 24, total predictions - 64\n",
      "minibatch - 28, loss - 0.6935421228408813, accurate predictions - 31, total predictions - 64\n",
      "minibatch - 29, loss - 0.6948975324630737, accurate predictions - 26, total predictions - 64\n",
      "minibatch - 30, loss - 0.6931765675544739, accurate predictions - 32, total predictions - 64\n",
      "minibatch - 31, loss - 0.6923338174819946, accurate predictions - 38, total predictions - 64\n",
      "minibatch - 32, loss - 0.6934264302253723, accurate predictions - 29, total predictions - 64\n",
      "minibatch - 33, loss - 0.6929595470428467, accurate predictions - 36, total predictions - 64\n",
      "minibatch - 34, loss - 0.69329833984375, accurate predictions - 9, total predictions - 24\n",
      "epoch - 5, loss - 0.7138941287994385\n",
      "minibatch - 0, loss - 0.6927803754806519, accurate predictions - 42, total predictions - 64\n",
      "minibatch - 1, loss - 0.692743718624115, accurate predictions - 35, total predictions - 64\n",
      "minibatch - 2, loss - 0.6929306387901306, accurate predictions - 33, total predictions - 64\n",
      "minibatch - 3, loss - 0.6917495727539062, accurate predictions - 36, total predictions - 64\n",
      "minibatch - 4, loss - 0.693731427192688, accurate predictions - 31, total predictions - 64\n",
      "minibatch - 5, loss - 0.6944773197174072, accurate predictions - 30, total predictions - 64\n",
      "minibatch - 6, loss - 0.6926752328872681, accurate predictions - 33, total predictions - 64\n",
      "minibatch - 7, loss - 0.692691445350647, accurate predictions - 33, total predictions - 64\n",
      "minibatch - 8, loss - 0.688689112663269, accurate predictions - 38, total predictions - 64\n",
      "minibatch - 9, loss - 0.6952979564666748, accurate predictions - 30, total predictions - 64\n",
      "minibatch - 10, loss - 0.6973876953125, accurate predictions - 28, total predictions - 64\n",
      "minibatch - 11, loss - 0.6936002373695374, accurate predictions - 32, total predictions - 64\n",
      "minibatch - 12, loss - 0.6945831179618835, accurate predictions - 31, total predictions - 64\n",
      "minibatch - 13, loss - 0.6896608471870422, accurate predictions - 36, total predictions - 64\n",
      "minibatch - 14, loss - 0.6936818361282349, accurate predictions - 32, total predictions - 64\n",
      "minibatch - 15, loss - 0.693649172782898, accurate predictions - 32, total predictions - 64\n",
      "minibatch - 16, loss - 0.6866182088851929, accurate predictions - 39, total predictions - 64\n",
      "minibatch - 17, loss - 0.696834146976471, accurate predictions - 29, total predictions - 64\n",
      "minibatch - 18, loss - 0.6979846358299255, accurate predictions - 28, total predictions - 64\n",
      "minibatch - 19, loss - 0.6936673521995544, accurate predictions - 32, total predictions - 64\n",
      "minibatch - 20, loss - 0.7005929946899414, accurate predictions - 25, total predictions - 64\n",
      "minibatch - 21, loss - 0.6970329284667969, accurate predictions - 28, total predictions - 64\n",
      "minibatch - 22, loss - 0.6904501914978027, accurate predictions - 36, total predictions - 64\n",
      "minibatch - 23, loss - 0.6907312273979187, accurate predictions - 36, total predictions - 64\n",
      "minibatch - 24, loss - 0.6974129676818848, accurate predictions - 25, total predictions - 64\n",
      "minibatch - 25, loss - 0.6932408809661865, accurate predictions - 32, total predictions - 64\n",
      "minibatch - 26, loss - 0.6949554085731506, accurate predictions - 27, total predictions - 64\n",
      "minibatch - 27, loss - 0.6931974291801453, accurate predictions - 32, total predictions - 64\n",
      "minibatch - 28, loss - 0.6930888891220093, accurate predictions - 36, total predictions - 64\n",
      "minibatch - 29, loss - 0.6928731799125671, accurate predictions - 33, total predictions - 64\n",
      "minibatch - 30, loss - 0.6944174766540527, accurate predictions - 30, total predictions - 64\n",
      "minibatch - 31, loss - 0.6939902901649475, accurate predictions - 31, total predictions - 64\n",
      "minibatch - 32, loss - 0.6874787211418152, accurate predictions - 41, total predictions - 64\n",
      "minibatch - 33, loss - 0.6997489333152771, accurate predictions - 27, total predictions - 64\n",
      "minibatch - 34, loss - 0.6966202259063721, accurate predictions - 11, total predictions - 24\n",
      "epoch - 6, loss - 0.7141548991203308\n",
      "minibatch - 0, loss - 0.6926636695861816, accurate predictions - 33, total predictions - 64\n",
      "minibatch - 1, loss - 0.6942954063415527, accurate predictions - 30, total predictions - 64\n",
      "minibatch - 2, loss - 0.6928002834320068, accurate predictions - 34, total predictions - 64\n",
      "minibatch - 3, loss - 0.6932678818702698, accurate predictions - 32, total predictions - 64\n",
      "minibatch - 4, loss - 0.692712664604187, accurate predictions - 33, total predictions - 64\n",
      "minibatch - 5, loss - 0.6933823823928833, accurate predictions - 32, total predictions - 64\n",
      "minibatch - 6, loss - 0.6964739561080933, accurate predictions - 27, total predictions - 64\n",
      "minibatch - 7, loss - 0.691573977470398, accurate predictions - 35, total predictions - 64\n",
      "minibatch - 8, loss - 0.6938024759292603, accurate predictions - 31, total predictions - 64\n",
      "minibatch - 9, loss - 0.6971961259841919, accurate predictions - 24, total predictions - 64\n",
      "minibatch - 10, loss - 0.6913408637046814, accurate predictions - 37, total predictions - 64\n",
      "minibatch - 11, loss - 0.6929649114608765, accurate predictions - 33, total predictions - 64\n",
      "minibatch - 12, loss - 0.6934069395065308, accurate predictions - 31, total predictions - 64\n",
      "minibatch - 13, loss - 0.6931027770042419, accurate predictions - 31, total predictions - 64\n",
      "minibatch - 14, loss - 0.6931424140930176, accurate predictions - 32, total predictions - 64\n",
      "minibatch - 15, loss - 0.6926451921463013, accurate predictions - 37, total predictions - 64\n",
      "minibatch - 16, loss - 0.6933771371841431, accurate predictions - 29, total predictions - 64\n",
      "minibatch - 17, loss - 0.6931917667388916, accurate predictions - 28, total predictions - 64\n",
      "minibatch - 18, loss - 0.6937997341156006, accurate predictions - 27, total predictions - 64\n",
      "minibatch - 19, loss - 0.6930001974105835, accurate predictions - 33, total predictions - 64\n",
      "minibatch - 20, loss - 0.6933833956718445, accurate predictions - 32, total predictions - 64\n",
      "minibatch - 21, loss - 0.6936283111572266, accurate predictions - 29, total predictions - 64\n",
      "minibatch - 22, loss - 0.6924930810928345, accurate predictions - 42, total predictions - 64\n",
      "minibatch - 23, loss - 0.6924706697463989, accurate predictions - 34, total predictions - 64\n",
      "minibatch - 24, loss - 0.6923686265945435, accurate predictions - 34, total predictions - 64\n",
      "minibatch - 25, loss - 0.692654550075531, accurate predictions - 33, total predictions - 64\n",
      "minibatch - 26, loss - 0.6939157247543335, accurate predictions - 32, total predictions - 64\n",
      "minibatch - 27, loss - 0.6984860897064209, accurate predictions - 28, total predictions - 64\n",
      "minibatch - 28, loss - 0.6935248970985413, accurate predictions - 32, total predictions - 64\n",
      "minibatch - 29, loss - 0.6949136257171631, accurate predictions - 30, total predictions - 64\n",
      "minibatch - 30, loss - 0.6949437856674194, accurate predictions - 28, total predictions - 64\n",
      "minibatch - 31, loss - 0.693528950214386, accurate predictions - 27, total predictions - 64\n",
      "minibatch - 32, loss - 0.6938416957855225, accurate predictions - 30, total predictions - 64\n",
      "minibatch - 33, loss - 0.6953422427177429, accurate predictions - 28, total predictions - 64\n",
      "minibatch - 34, loss - 0.6906353235244751, accurate predictions - 14, total predictions - 24\n",
      "epoch - 7, loss - 0.7139492630958557\n",
      "minibatch - 0, loss - 0.6924382448196411, accurate predictions - 34, total predictions - 64\n",
      "minibatch - 1, loss - 0.6953059434890747, accurate predictions - 29, total predictions - 64\n",
      "minibatch - 2, loss - 0.6914297938346863, accurate predictions - 35, total predictions - 64\n",
      "minibatch - 3, loss - 0.69402015209198, accurate predictions - 31, total predictions - 64\n",
      "minibatch - 4, loss - 0.6928321123123169, accurate predictions - 33, total predictions - 64\n",
      "minibatch - 5, loss - 0.6933917999267578, accurate predictions - 32, total predictions - 64\n",
      "minibatch - 6, loss - 0.6933318376541138, accurate predictions - 32, total predictions - 64\n",
      "minibatch - 7, loss - 0.6933849453926086, accurate predictions - 32, total predictions - 64\n",
      "minibatch - 8, loss - 0.6921555995941162, accurate predictions - 34, total predictions - 64\n",
      "minibatch - 9, loss - 0.6889936923980713, accurate predictions - 39, total predictions - 64\n",
      "minibatch - 10, loss - 0.6955780982971191, accurate predictions - 29, total predictions - 64\n",
      "minibatch - 11, loss - 0.6934736967086792, accurate predictions - 32, total predictions - 64\n",
      "minibatch - 12, loss - 0.6949408650398254, accurate predictions - 30, total predictions - 64\n",
      "minibatch - 13, loss - 0.6977270841598511, accurate predictions - 26, total predictions - 64\n",
      "minibatch - 14, loss - 0.6940106153488159, accurate predictions - 31, total predictions - 64\n",
      "minibatch - 15, loss - 0.6959719061851501, accurate predictions - 27, total predictions - 64\n",
      "minibatch - 16, loss - 0.6936872005462646, accurate predictions - 31, total predictions - 64\n",
      "minibatch - 17, loss - 0.6932265758514404, accurate predictions - 32, total predictions - 64\n",
      "minibatch - 18, loss - 0.6920590400695801, accurate predictions - 37, total predictions - 64\n",
      "minibatch - 19, loss - 0.6926075220108032, accurate predictions - 35, total predictions - 64\n",
      "minibatch - 20, loss - 0.6949015855789185, accurate predictions - 21, total predictions - 64\n",
      "minibatch - 21, loss - 0.6930411458015442, accurate predictions - 33, total predictions - 64\n",
      "minibatch - 22, loss - 0.6930135488510132, accurate predictions - 34, total predictions - 64\n",
      "minibatch - 23, loss - 0.6931385397911072, accurate predictions - 32, total predictions - 64\n",
      "minibatch - 24, loss - 0.6932093501091003, accurate predictions - 27, total predictions - 64\n",
      "minibatch - 25, loss - 0.6930607557296753, accurate predictions - 34, total predictions - 64\n",
      "minibatch - 26, loss - 0.6930729150772095, accurate predictions - 33, total predictions - 64\n",
      "minibatch - 27, loss - 0.6935714483261108, accurate predictions - 28, total predictions - 64\n",
      "minibatch - 28, loss - 0.6935517191886902, accurate predictions - 29, total predictions - 64\n",
      "minibatch - 29, loss - 0.6927239894866943, accurate predictions - 35, total predictions - 64\n",
      "minibatch - 30, loss - 0.6950307488441467, accurate predictions - 20, total predictions - 64\n",
      "minibatch - 31, loss - 0.6926008462905884, accurate predictions - 38, total predictions - 64\n",
      "minibatch - 32, loss - 0.6928273439407349, accurate predictions - 36, total predictions - 64\n",
      "minibatch - 33, loss - 0.6938101053237915, accurate predictions - 23, total predictions - 64\n",
      "minibatch - 34, loss - 0.6932185888290405, accurate predictions - 12, total predictions - 24\n",
      "epoch - 8, loss - 0.7138628363609314\n",
      "minibatch - 0, loss - 0.69319087266922, accurate predictions - 31, total predictions - 64\n",
      "minibatch - 1, loss - 0.6934730410575867, accurate predictions - 24, total predictions - 64\n",
      "minibatch - 2, loss - 0.6930212378501892, accurate predictions - 35, total predictions - 64\n",
      "minibatch - 3, loss - 0.6928955316543579, accurate predictions - 35, total predictions - 64\n",
      "minibatch - 4, loss - 0.6928106546401978, accurate predictions - 34, total predictions - 64\n",
      "minibatch - 5, loss - 0.6934182047843933, accurate predictions - 30, total predictions - 64\n",
      "minibatch - 6, loss - 0.6942808628082275, accurate predictions - 26, total predictions - 64\n",
      "minibatch - 7, loss - 0.6924307346343994, accurate predictions - 36, total predictions - 64\n",
      "minibatch - 8, loss - 0.6938362717628479, accurate predictions - 29, total predictions - 64\n",
      "minibatch - 9, loss - 0.6925359964370728, accurate predictions - 35, total predictions - 64\n",
      "minibatch - 10, loss - 0.692821741104126, accurate predictions - 34, total predictions - 64\n",
      "minibatch - 11, loss - 0.6933255195617676, accurate predictions - 31, total predictions - 64\n",
      "minibatch - 12, loss - 0.6921990513801575, accurate predictions - 36, total predictions - 64\n",
      "minibatch - 13, loss - 0.6932374835014343, accurate predictions - 32, total predictions - 64\n",
      "minibatch - 14, loss - 0.6941314339637756, accurate predictions - 29, total predictions - 64\n",
      "minibatch - 15, loss - 0.6954073905944824, accurate predictions - 25, total predictions - 64\n",
      "minibatch - 16, loss - 0.6925143599510193, accurate predictions - 34, total predictions - 64\n",
      "minibatch - 17, loss - 0.6934434175491333, accurate predictions - 31, total predictions - 64\n",
      "minibatch - 18, loss - 0.6913653612136841, accurate predictions - 41, total predictions - 64\n",
      "minibatch - 19, loss - 0.6935416460037231, accurate predictions - 30, total predictions - 64\n",
      "minibatch - 20, loss - 0.6931047439575195, accurate predictions - 32, total predictions - 64\n",
      "minibatch - 21, loss - 0.6924278736114502, accurate predictions - 35, total predictions - 64\n",
      "minibatch - 22, loss - 0.694564163684845, accurate predictions - 27, total predictions - 64\n",
      "minibatch - 23, loss - 0.6927962303161621, accurate predictions - 33, total predictions - 64\n",
      "minibatch - 24, loss - 0.6926219463348389, accurate predictions - 34, total predictions - 64\n",
      "minibatch - 25, loss - 0.6933692693710327, accurate predictions - 31, total predictions - 64\n",
      "minibatch - 26, loss - 0.6932525634765625, accurate predictions - 32, total predictions - 64\n",
      "minibatch - 27, loss - 0.6944053173065186, accurate predictions - 28, total predictions - 64\n",
      "minibatch - 28, loss - 0.6932131052017212, accurate predictions - 31, total predictions - 64\n",
      "minibatch - 29, loss - 0.6925005912780762, accurate predictions - 35, total predictions - 64\n",
      "minibatch - 30, loss - 0.6927521228790283, accurate predictions - 34, total predictions - 64\n",
      "minibatch - 31, loss - 0.6932935118675232, accurate predictions - 31, total predictions - 64\n",
      "minibatch - 32, loss - 0.6922594904899597, accurate predictions - 37, total predictions - 64\n",
      "minibatch - 33, loss - 0.6932279467582703, accurate predictions - 31, total predictions - 64\n",
      "minibatch - 34, loss - 0.6965541243553162, accurate predictions - 6, total predictions - 24\n",
      "epoch - 9, loss - 0.7136535048484802\n"
     ]
    }
   ],
   "source": [
    "model = SiameseNetwork(3, 0.1)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.002)\n",
    "n_epochs = 10\n",
    "model.to(device)\n",
    "for epoch in range(n_epochs):\n",
    "    running_loss = 0\n",
    "    accurate_predictions_tot = 0\n",
    "    total_predictions_tot = 0\n",
    "    for idx, (image1, image2, labels) in enumerate(train_dataloader):\n",
    "        image1 = image1.to(device)\n",
    "        image2 = image2.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        y_preds = model(image1, image2)\n",
    "        loss = criterion(y_preds.squeeze(), labels.to(torch.float32))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.detach()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_preds_class = (y_preds >= 0.5).int()\n",
    "            accurate_predictions = (y_preds_class == labels.reshape(-1, 1)).sum()\n",
    "            total_predictions = labels.shape[0]\n",
    "            accurate_predictions_tot += accurate_predictions\n",
    "            total_predictions_tot += total_predictions\n",
    "            print(f\"minibatch - {idx}, loss - {loss}, accurate predictions - {accurate_predictions}, total predictions - {total_predictions}\")\n",
    "    \n",
    "    print(f\"epoch - {epoch}, loss - {running_loss/idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "c0166907-6e56-4942-990b-30b0ff14e3ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3818e+10, device='cuda:0')"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d33eed87-01f9-4b07-970b-37277f4a6909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d4831429-1c17-4232-85f3-568f2da350d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24, 3, 250, 250])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "cb54620c-5bcd-4305-a8fc-476fe4ce1a15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24, 1])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1f55807b-1365-49b9-bf4a-3a721ab89a2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24, 1])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_preds_class.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f2c83e-34e7-44a3-8a82-1535a6fa414e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea3e257-4ded-463c-b3c5-94a1263cc53d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b09ee5-310d-48b5-80c9-856cfab3bcb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "9532199a-9be1-4805-a997-67e545abcc2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch - 0, loss - 0.43223831057548523\n",
      "epoch - 1, loss - nan\n",
      "epoch - 2, loss - nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[205], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# correctly_classified = 0\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# total_classified = 0\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage1\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimage1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimage2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.virtualenvs\\FaceRecognition-ikLH-k2n\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\.virtualenvs\\FaceRecognition-ikLH-k2n\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\.virtualenvs\\FaceRecognition-ikLH-k2n\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\.virtualenvs\\FaceRecognition-ikLH-k2n\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[99], line 52\u001b[0m, in \u001b[0;36mLFWDatasetSlow.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     50\u001b[0m     pairs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpairs_data\u001b[38;5;241m.\u001b[39mpairs_data_dev\n\u001b[0;32m     51\u001b[0m pair \u001b[38;5;241m=\u001b[39m pairs_list[idx]\n\u001b[1;32m---> 52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_image_label_pair\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpair\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[99], line 42\u001b[0m, in \u001b[0;36mLFWDatasetSlow._get_image_label_pair\u001b[1;34m(self, pair)\u001b[0m\n\u001b[0;32m     40\u001b[0m img_2 \u001b[38;5;241m=\u001b[39m img_2\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m250\u001b[39m\n\u001b[0;32m     41\u001b[0m img_1 \u001b[38;5;241m=\u001b[39m img_1\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m---> 42\u001b[0m img_2 \u001b[38;5;241m=\u001b[39m img_2\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img_1, img_2, label\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = SiameseNetwork(3, 0.1)\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "n_epochs = 50\n",
    "model.to(device)\n",
    "model.train()\n",
    "for epoch in range(n_epochs):\n",
    "    running_loss = 0\n",
    "    # correctly_classified = 0\n",
    "    # total_classified = 0\n",
    "    for idx, (image1, image2, labels) in enumerate(train_dataloader):\n",
    "        image1 = image1.to(device)\n",
    "        image2 = image2.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        img1_emb, img2_emb = model(image1, image2)\n",
    "        loss = model.loss(img1_emb, img2_emb, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss = (1 - 1/(idx + 1)) * running_loss + (1/(idx + 1))*loss.detach()\n",
    "        # model.eval()\n",
    "        # with torch.no_grad():\n",
    "        #     y_pred = model.cosine_similarity(img1_emb, img2_emb)\n",
    "        #     y_pred_labels = (y_pred >= 0.8).float()\n",
    "        #     correctly_classified += (y_pred_labels == labels).sum()\n",
    "        #     total_classified += labels.shape[0] \n",
    "    print(f\"epoch - {epoch}, loss - {running_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "1af4e552-d11a-4a99-a9cf-256d28b5b314",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(sci_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "45e340c5-e6ee-417c-91ef-d49360dbc3d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([     0.2087,      0.0405,      0.0579,      0.0562,      0.0156,\n",
       "             0.1592,      0.1275,      0.1880,     -0.2900,      0.0466,\n",
       "             0.0644,     -0.1400,      0.0448,      0.1228,     -0.1514,\n",
       "             0.3569,      0.0361,     -0.0018,      0.5856,      0.0746,\n",
       "             0.2159,     -0.0920,     -0.0996,      0.2665,      0.1758,\n",
       "            -0.0734,      0.2963,      0.1449,     -0.0832,     -0.2128,\n",
       "             0.1015,     -0.0795,      0.0229,     -0.2411,     -0.0988,\n",
       "             0.0453,     -0.0463,      0.0370,     -0.0728,      0.0015,\n",
       "            -0.1661,     -0.0042,     -0.0573,      0.0214,     -0.1041,\n",
       "             0.0677,      0.3715,     -0.0792,     -0.1005,      0.0515,\n",
       "             0.1049,      0.1055,      0.0482,     -0.0653,     -0.1535,\n",
       "             0.1385,      0.0001,      0.0468,      0.1537,     -0.0759,\n",
       "            -0.1166,      0.0403,      0.0225,      0.0866], device='cuda:0')"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "5c7d4093-d279-4559-973e-100f234efd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inference_dataloader = DataLoader(train_lfw_data, batch_size = 64, shuffle = False)\n",
    "# dev_dataloader = DataLoader(dev_lfw_data, batch_size = 64, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df8cf97-adc9-4dbd-8259-5769ca98d466",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "6eb7160a-efce-4099-bace-76bf55d4e367",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = []\n",
    "all_labels = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for img1, img2, labels in train_inference_dataloader:\n",
    "        img1 = img1.to(device)\n",
    "        img2 = img2.to(device)\n",
    "        img1_emb, img2_emb = model(img1, img2)\n",
    "        y_pred = model.cosine_similarity_adj(img1_emb, img2_emb)\n",
    "        y_preds.append(y_pred.detach())\n",
    "        all_labels.append(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "e2ba47e9-4359-4c82-9956-edf1fe8159a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = torch.hstack(y_preds)\n",
    "true_labels = torch.hstack(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "dabfb4df-b499-4a2d-9452-1caf8de5096d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = y_pred_proba.to('cpu').numpy()\n",
    "true_labels = true_labels.to('cpu').numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "2ce7b1bd-63e3-404b-8c06-cbf95fff5340",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "045d471a-3eb1-4424-b8ef-a3e56d7e9ada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2200,)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "83405572-b261-4173-ac7f-17ddd08a2596",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds.tofile('./y_preds', sep = \" \")\n",
    "true_labels.tofile(\"./true_labels\", sep = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "cb7f14c4-dba4-43f7-9ff5-17cbf2dbcbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds_test = np.fromfile('./y_preds', sep = \" \")\n",
    "true_labels = np.fromfile('./true_labels', sep = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "deb51883-612f-47e4-9f2c-a013e6ea5e15",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[147], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e4bb00-e2c4-4223-ad43-adae5b2c0559",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_preds_test[true_labels == 1], bins = 45)\n",
    "plt.hist(y_preds_test[true_labels == 0], bins = 45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "75b9073e-7553-45e7-9638-ec9ed0e669ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_preds, threshold, true_labels):\n",
    "    y_pred_labels = (y_preds > threshold).astype(int)\n",
    "    return sum(y_pred_labels == true_labels)/len(true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "c9504691-13f6-40d6-a0f9-ffa13c030734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(y_preds_test, 0.1, true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "fef26f29-c81a-49ce-a661-52eba7c2cb5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.495"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(y_preds_test, 0.2, true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "c40f75c1-83ee-410c-a471-cc56bcc86cab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4827272727272727"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(y_preds_test, 0.3, true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "4f575ee7-e1e2-4727-ba3c-8819f9302243",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(y_preds_test, 0.5, true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "368640a6-c9fd-405d-94e3-75222099c796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5036363636363637"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(y_preds_test, 0.9, true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811e74dd-c495-4dd8-9b6b-eedf4d9cf949",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c86cd2e-bddb-4c26-a2fa-60219a4cbb91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d271d14-f4c8-42ee-af4b-e96e92d5751c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c12ac52-1c2c-4450-9c85-4043660817e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47dd2a5-8013-4951-a598-99cae8c4cf84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595f8beb-2985-4584-8b78-70ab06a46579",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a310018e-ad13-49f8-9e0f-30884e7a8fe2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372ac651-332d-49c4-91a2-d5038526650d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06580c4-0d6a-425f-8f15-0f4854297a03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddcbe50-2c80-4c56-bdad-31b5e009b22c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66a36f0-3033-4f1f-a40f-17cd685511e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74c1237-59cd-47d1-a626-39d90cbcb4d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a451eba2-f164-4aac-82ab-cc94aa147fd1",
   "metadata": {},
   "source": [
    "#### Optimizer pytorch guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66424d97-52d1-4389-9caf-27b97ae5fda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum = 0.9)\n",
    "optimizer = optim.Adam([var1, var2], lr = 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccea9f5-774c-4c9b-8201-2fd2c1dc6f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim.SGD([\n",
    "                {'params': model.base.parameters()},\n",
    "                {'params': model.classifier.parameters(), 'lr': 1e-3}\n",
    "            ], lr=1e-2, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b1a0a8-0a68-410c-93a7-f652a3273a36",
   "metadata": {},
   "source": [
    "- The first input to an optimizer is an iterable consisting of all the parameters that we want the optimizer to handle. The parameters are all of the type Variable.\n",
    "- We can also divide the parameters into different groups where each group can have its own optimizer configuration like learning rate, momentum etc. We can also send in a master list of parameters which will act as the default for groups without an override.\n",
    "- There are learning rate schedulers as well. These are usually applied similar to how an optimizer is applied, using the step function.\n",
    "- These are usually applied after each epoch (why? Is it because applying after each mini batch breaks some sort of estimation?) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d906fda5-e7fd-4dc3-abe6-b4fa917223cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ExponentialLR        # We can import learning rate schedulers which update after every epoch  \n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau    # This particular scheduler is more like a conditional one which is applied after out performance improves beyond some threshold\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6cab9b-0262-4a1f-8ac9-8eff5a29aeb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd30727-f49e-46b9-a3e7-92c0ed00895d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9b8755-fc73-4aef-a6c7-28577d60a583",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a431635d-4842-4522-a252-1f2a1d741202",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb020d95-7bd5-4b63-b252-9611a6069a76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53c4477-9b47-436c-b9f1-b454f171550b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719e9bd9-7c33-438d-add5-bc0574918226",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fb852f-4ceb-4e62-99cf-43fb69eaa90d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b5c41a-61fc-497f-8f37-e3d3213e47e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf76c96-5812-4bf1-9b2f-dde952052500",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0419bd07-8503-4ffb-8480-bffe8c402dbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ccc742-725d-4d74-b5f5-0f70313e6118",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a39fb8-534d-4f5f-83dd-5abc07eadccb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19df347e-d2c5-4472-86b7-05769b4e1c34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f306c8-4a90-4b56-9904-343a31ab433d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfffe87-faa6-4b29-926e-74a6bc845487",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa85f65-1819-48bd-aa32-fe4ccd398838",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbca0a23-df8e-4205-9946-ad47bd7bb9d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27732ca-c731-48e7-94cb-2848d3125f54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33405ae8-404e-40b7-90ea-040c97ffd8f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
